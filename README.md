## Generative Text Model

This notebook demonstrates a simple text generation application using the GPT-2 model from HuggingFace Transformers.

## ðŸš€ How to Run
1. Open the notebook in Jupyter or Google Colab.
2. Install dependencies and run all cells.
3. Enter any topic when prompted, and the model will generate a paragraph.

## ðŸ›  Tech Stack
- Python
- Transformers Library
- Pretrained GPT-2 Model

## ðŸ”— Resources
- https://huggingface.co/gpt2
